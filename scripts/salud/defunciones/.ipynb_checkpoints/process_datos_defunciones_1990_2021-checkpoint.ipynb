{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2b0d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cd1f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers for CSV files without headers\n",
    "DEFAULT_HEADERS = [\n",
    "\"AÃ‘O\",\"FECHA_DEF\",\"SEXO_NOMBRE\",\"EDAD_TIPO\",\"EDAD_CANT\",\"COD_COMUNA\",\"COMUNA\",\"NOMBRE_REGION\",\"DIAG1\",\"CAPITULO_DIAG1\",\"GLOSA_CAPITULO_DIAG1\",\"CODIGO_GRUPO_DIAG1\",\"GLOSA_GRUPO_DIAG1\",\"CODIGO_CATEGORIA_DIAG1\",\"GLOSA_CATEGORIA_DIAG1\",\"CODIGO_SUBCATEGORIA_DIAG1\",\"GLOSA_SUBCATEGORIA_DIAG1\",\"DIAG2\",\"CAPITULO_DIAG2\",\"GLOSA_CAPITULO_DIAG2\",\"CODIGO_GRUPO_DIAG2\",\"GLOSA_GRUPO_DIAG2\",\"CODIGO_CATEGORIA_DIAG2\",\"GLOSA_CATEGORIA_DIAG2\",\"CODIGO_SUBCATEGORIA_DIAG2\",\"GLOSA_SUBCATEGORIA_DIAG2\",\"LUGAR_DEFUNCION\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64398458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths():\n",
    "    # Check if running in the GitHub Actions environment\n",
    "    if 'GITHUB_ACTIONS' in os.environ:\n",
    "        base_path = os.getcwd()\n",
    "    else:\n",
    "        # Assuming your notebook is in the 'scripts' directory\n",
    "        base_path = os.path.abspath(os.path.join(os.getcwd(), '../../../data'))\n",
    "\n",
    "    source_path = os.path.join(base_path, \"source/minsal/deis\")\n",
    "    processed_path = os.path.join(base_path, \"processed/minsal/deis\")\n",
    "    \n",
    "    return source_path, processed_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "25bcb2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source dir: /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/source/minsal/deis\n",
      "Processed dir: /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/minsal/deis\n"
     ]
    }
   ],
   "source": [
    "source_dir, processed_dir = get_data_paths()\n",
    "\n",
    "print(f\"Source dir: {source_dir}\")\n",
    "print(f\"Processed dir: {processed_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e5cff364",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(source_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a423cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file URLs and corresponding CSV filenames to extract\n",
    "file_info = [\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip\", \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv\"),\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip\", \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08e13b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a file from a URL\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download file from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90c87a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file_path, delimiter=';', encoding='utf-8', header='infer'):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep=delimiter, encoding=encoding, index_col=False, low_memory=False, header=header)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Encoding error with {file_path}, trying ISO-8859-1\")\n",
    "        return pd.read_csv(file_path, sep=delimiter, encoding='ISO-8859-1', index_col=False, low_memory=False, header=header)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f473132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_csv_from_zip(zip_path, extract_filename, delimiter=';', encoding='utf-8', header='infer'):\n",
    "    with ZipFile(zip_path, 'r') as zip_file:\n",
    "        if extract_filename in zip_file.namelist():\n",
    "            with zip_file.open(extract_filename) as f:\n",
    "                return read_csv_file(f, delimiter=delimiter, encoding=encoding, header=header)\n",
    "        else:\n",
    "            print(f\"{extract_filename} not found in the zip archive.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "482fc3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_zip_file(url, extract_filename, source_dir, processed_dir, delimiter=';', header='infer'):\n",
    "    # Extract filename from URL\n",
    "    zip_filename = url.split('/')[-1]\n",
    "    zip_path = os.path.join(source_dir, zip_filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(zip_path):\n",
    "        # Download the zip file if it doesn't exist\n",
    "        download_file(url, zip_path)\n",
    "        print(f\"Downloaded {zip_filename}\")\n",
    "    else:\n",
    "        print(f\"{zip_filename} already exists. Skipping download.\")\n",
    "    \n",
    "    # Extract and process the CSV file\n",
    "    df = extract_csv_from_zip(zip_path, extract_filename, delimiter=delimiter, header=header)\n",
    "    if df is not None:\n",
    "        parquet_filename = extract_filename.replace('.csv', '.parquet')\n",
    "        parquet_path = os.path.join(processed_dir, parquet_filename)\n",
    "        df.to_parquet(parquet_path)\n",
    "        print(f\"Processed {extract_filename} and saved to {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65070bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e767ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CSV to Parquet\n",
    "def convert_csv_to_parquet(csv_path, parquet_path):\n",
    "    df = read_csv_file(csv_path)\n",
    "    df.to_parquet(parquet_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7c4d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip already exists. Skipping download.\n",
      "Encoding error with <zipfile.ZipExtFile name='DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv' mode='r' compress_type=deflate>, trying ISO-8859-1\n",
      "Processed DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv and saved to /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/minsal/deis/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\n",
      "DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip already exists. Skipping download.\n",
      "Encoding error with <zipfile.ZipExtFile name='DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv' mode='r' compress_type=deflate>, trying ISO-8859-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/swywv8hn21b35kfmmssjg33m0000gn/T/ipykernel_83801/165257477.py:6: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return pd.read_csv(file_path, sep=delimiter, encoding='ISO-8859-1', index_col=False, low_memory=False, header=header)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 19 fields in line 2, saw 27\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m, in \u001b[0;36mread_csv_file\u001b[0;34m(file_path, delimiter, encoding, header)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39mencoding, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39mheader)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:782\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd1 in position 1: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, extract_filename \u001b[38;5;129;01min\u001b[39;00m file_info:\n\u001b[1;32m      3\u001b[0m     header_option \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1990_2021\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m extract_filename \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     process_zip_file(url, extract_filename, source_dir, processed_dir, header\u001b[38;5;241m=\u001b[39mheader_option, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[47], line 15\u001b[0m, in \u001b[0;36mprocess_zip_file\u001b[0;34m(url, extract_filename, source_dir, processed_dir, delimiter, header)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Skipping download.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Extract and process the CSV file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df \u001b[38;5;241m=\u001b[39m extract_csv_from_zip(zip_path, extract_filename, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, header\u001b[38;5;241m=\u001b[39mheader)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     parquet_filename \u001b[38;5;241m=\u001b[39m extract_filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m, in \u001b[0;36mextract_csv_from_zip\u001b[0;34m(zip_path, extract_filename, delimiter, encoding, header)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_filename \u001b[38;5;129;01min\u001b[39;00m zip_file\u001b[38;5;241m.\u001b[39mnamelist():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m zip_file\u001b[38;5;241m.\u001b[39mopen(extract_filename) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m read_csv_file(f, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39mencoding, header\u001b[38;5;241m=\u001b[39mheader)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the zip archive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m, in \u001b[0;36mread_csv_file\u001b[0;34m(file_path, delimiter, encoding, header)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding error with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, trying ISO-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39mheader)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 19 fields in line 2, saw 27\n"
     ]
    }
   ],
   "source": [
    "# Process each zip file\n",
    "for url, extract_filename in file_info:\n",
    "    header_option = None if '1990_2021' in extract_filename else 'infer'\n",
    "    process_zip_file(url, extract_filename, source_dir, processed_dir, header=header_option, delimiter=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and merge both files using the common structure\n",
    "df1 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\"))\n",
    "df2 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.parquet\"))\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save combined dataframe\n",
    "combined_parquet_path = os.path.join(processed_dir, \"combined_defunciones.parquet\")\n",
    "df_combined.to_parquet(combined_parquet_path)\n",
    "print(f\"Combined DataFrame saved to {combined_parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "244a37ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip already exists. Skipping download.\n",
      "Number of columns detected: 25\n",
      "Encoding error with <zipfile.ZipExtFile name='DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv' mode='r' compress_type=deflate>, trying ISO-8859-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_h/swywv8hn21b35kfmmssjg33m0000gn/T/ipykernel_83801/1693677727.py:45: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return pd.read_csv(file_path, sep=delimiter, encoding='ISO-8859-1', index_col=False, low_memory=False, header=header, names=names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv and saved to /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/vitales/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\n",
      "DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip already exists. Skipping download.\n",
      "Number of columns detected: 26\n",
      "Mismatch in number of columns between headers and data.\n",
      "Encoding error with <zipfile.ZipExtFile name='DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv' mode='r' compress_type=deflate>, trying ISO-8859-1\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 19 fields in line 2, saw 27\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 40\u001b[0m, in \u001b[0;36mread_csv_file\u001b[0;34m(file_path, delimiter, encoding, header, names)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39mencoding, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully read file with encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:782\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd1 in position 1: invalid continuation byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m     header_option \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022_2024\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m extract_filename \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     names_option \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m header_option \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m DEFAULT_HEADERS\n\u001b[0;32m--> 110\u001b[0m     process_zip_file(url, extract_filename, source_dir, processed_dir, header\u001b[38;5;241m=\u001b[39mheader_option, names\u001b[38;5;241m=\u001b[39mnames_option)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Read and merge both files using the common structure\u001b[39;00m\n\u001b[1;32m    113\u001b[0m df1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(processed_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[0;32mIn[55], line 88\u001b[0m, in \u001b[0;36mprocess_zip_file\u001b[0;34m(url, extract_filename, source_dir, processed_dir, delimiter, header, names)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Skipping download.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Extract and process the CSV file\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m df \u001b[38;5;241m=\u001b[39m extract_csv_from_zip(zip_path, extract_filename, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     parquet_filename \u001b[38;5;241m=\u001b[39m extract_filename\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 65\u001b[0m, in \u001b[0;36mextract_csv_from_zip\u001b[0;34m(zip_path, extract_filename, delimiter, header, names)\u001b[0m\n\u001b[1;32m     63\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_csv_file(f, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_csv_file(f, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n",
      "Cell \u001b[0;32mIn[55], line 45\u001b[0m, in \u001b[0;36mread_csv_file\u001b[0;34m(file_path, delimiter, encoding, header, names)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding error with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, trying ISO-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:239\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_chunk:\n",
      "File \u001b[0;32mparsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:914\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 19 fields in line 2, saw 27\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "\n",
    "# Define headers for CSV files without headers\n",
    "DEFAULT_HEADERS = [\n",
    "    \"AÃ‘O\", \"FECHA_DEF\", \"SEXO_NOMBRE\", \"EDAD_TIPO\", \"EDAD_CANT\", \"COD_COMUNA\", \"COMUNA\", \"NOMBRE_REGION\",\n",
    "    \"DIAG1\", \"CAPITULO_DIAG1\", \"GLOSA_CAPITULO_DIAG1\", \"CODIGO_GRUPO_DIAG1\", \"GLOSA_GRUPO_DIAG1\",\n",
    "    \"CODIGO_CATEGORIA_DIAG1\", \"GLOSA_CATEGORIA_DIAG1\", \"CODIGO_SUBCATEGORIA_DIAG1\", \"GLOSA_SUBCATEGORIA_DIAG1\",\n",
    "    \"DIAG2\", \"CAPITULO_DIAG2\", \"GLOSA_CAPITULO_DIAG2\", \"CODIGO_GRUPO_DIAG2\", \"GLOSA_GRUPO_DIAG2\",\n",
    "    \"CODIGO_CATEGORIA_DIAG2\", \"GLOSA_CATEGORIA_DIAG2\", \"CODIGO_SUBCATEGORIA_DIAG2\", \"GLOSA_SUBCATEGORIA_DIAG2\",\n",
    "    \"LUGAR_DEFUNCION\"\n",
    "]\n",
    "\n",
    "def get_data_paths():\n",
    "    # Check if running in the GitHub Actions environment\n",
    "    if 'GITHUB_ACTIONS' in os.environ:\n",
    "        base_path = os.getcwd()\n",
    "    else:\n",
    "        # Assuming your notebook is in the 'scripts' directory\n",
    "        base_path = os.path.abspath(os.path.join(os.getcwd(), '../../../'))\n",
    "\n",
    "    source_path = os.path.join(base_path, \"data/source/vitales\")\n",
    "    processed_path = os.path.join(base_path, \"data/processed/vitales\")\n",
    "    \n",
    "    return source_path, processed_path\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download file from {url}\")\n",
    "\n",
    "def read_csv_file(file_path, delimiter=';', encoding='utf-8', header='infer', names=None):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=delimiter, encoding=encoding, index_col=False, low_memory=False, header=header, names=names)\n",
    "        print(f\"Successfully read file with encoding {encoding}\")\n",
    "        return df\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Encoding error with {file_path}, trying ISO-8859-1\")\n",
    "        return pd.read_csv(file_path, sep=delimiter, encoding='ISO-8859-1', index_col=False, low_memory=False, header=header, names=names)\n",
    "\n",
    "def extract_csv_from_zip(zip_path, extract_filename, delimiter=';', header='infer', names=None):\n",
    "    with ZipFile(zip_path, 'r') as zip_file:\n",
    "        if extract_filename in zip_file.namelist():\n",
    "            with zip_file.open(extract_filename) as f:\n",
    "                try:\n",
    "                    first_lines = f.read(2048).decode('utf-8').split('\\n')\n",
    "                except UnicodeDecodeError:\n",
    "                    first_lines = f.read(2048).decode('ISO-8859-1').split('\\n')\n",
    "                \n",
    "                num_columns = len(first_lines[0].split(delimiter))\n",
    "                print(f\"Number of columns detected: {num_columns}\")\n",
    "                if names and len(names) != num_columns:\n",
    "                    print(\"Mismatch in number of columns between headers and data.\")\n",
    "                    names = None  # Fall back to automatic handling\n",
    "                \n",
    "                # Reset the file pointer and read the entire file\n",
    "                f.seek(0)\n",
    "                try:\n",
    "                    df = read_csv_file(f, delimiter=delimiter, encoding='utf-8', header=header, names=names)\n",
    "                except UnicodeDecodeError:\n",
    "                    df = read_csv_file(f, delimiter=delimiter, encoding='ISO-8859-1', header=header, names=names)\n",
    "                \n",
    "                return df\n",
    "        else:\n",
    "            print(f\"{extract_filename} not found in the zip archive.\")\n",
    "            return None\n",
    "\n",
    "def process_zip_file(url, extract_filename, source_dir, processed_dir, delimiter=';', header='infer', names=None):\n",
    "    # Extract filename from URL\n",
    "    zip_filename = url.split('/')[-1]\n",
    "    zip_path = os.path.join(source_dir, zip_filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(zip_path):\n",
    "        # Download the zip file if it doesn't exist\n",
    "        download_file(url, zip_path)\n",
    "        print(f\"Downloaded {zip_filename}\")\n",
    "    else:\n",
    "        print(f\"{zip_filename} already exists. Skipping download.\")\n",
    "    \n",
    "    # Extract and process the CSV file\n",
    "    df = extract_csv_from_zip(zip_path, extract_filename, delimiter=delimiter, header=header, names=names)\n",
    "    if df is not None:\n",
    "        parquet_filename = extract_filename.replace('.csv', '.parquet')\n",
    "        parquet_path = os.path.join(processed_dir, parquet_filename)\n",
    "        df.to_parquet(parquet_path)\n",
    "        print(f\"Processed {extract_filename} and saved to {parquet_path}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "source_dir, processed_dir = get_data_paths()\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# List of file URLs and corresponding CSV filenames to extract\n",
    "file_info = [\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip\", \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv\"),\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip\", \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv\")\n",
    "]\n",
    "\n",
    "# Process each zip file\n",
    "for url, extract_filename in file_info:\n",
    "    header_option = 'infer' if '2022_2024' in extract_filename else None\n",
    "    names_option = None if header_option == 'infer' else DEFAULT_HEADERS\n",
    "    process_zip_file(url, extract_filename, source_dir, processed_dir, header=header_option, names=names_option)\n",
    "\n",
    "# Read and merge both files using the common structure\n",
    "df1 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\"))\n",
    "df2 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.parquet\"))\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save combined dataframe\n",
    "combined_parquet_path = os.path.join(processed_dir, \"combined_defunciones.parquet\")\n",
    "df_combined.to_parquet(combined_parquet_path)\n",
    "print(f\"Combined DataFrame saved to {combined_parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2094ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_csv_from_zip(zip_path, extract_filename, delimiter=delimiter, header=header, names=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f6dd524",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = os.path.join(source_dir, \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c7fdba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/source/vitales/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52f9ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_filename = \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0c5b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "098cfa59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns detected: 25\n",
      "Mismatch in number of columns between headers and data.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "header must be integer or list of integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m extract_csv_from_zip(zip_path, extract_filename, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, names\u001b[38;5;241m=\u001b[39mDEFAULT_HEADERS)\n",
      "Cell \u001b[0;32mIn[55], line 65\u001b[0m, in \u001b[0;36mextract_csv_from_zip\u001b[0;34m(zip_path, extract_filename, delimiter, header, names)\u001b[0m\n\u001b[1;32m     63\u001b[0m f\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_csv_file(f, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     df \u001b[38;5;241m=\u001b[39m read_csv_file(f, delimiter\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n",
      "Cell \u001b[0;32mIn[55], line 40\u001b[0m, in \u001b[0;36mread_csv_file\u001b[0;34m(file_path, delimiter, encoding, header, names)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv_file\u001b[39m(file_path, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, sep\u001b[38;5;241m=\u001b[39mdelimiter, encoding\u001b[38;5;241m=\u001b[39mencoding, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39mheader, names\u001b[38;5;241m=\u001b[39mnames)\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully read file with encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1614\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_file_or_buffer(f, engine)\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_options(options, engine)\n\u001b[1;32m   1616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwds:\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1787\u001b[0m, in \u001b[0;36mTextFileReader._clean_options\u001b[0;34m(self, options, engine)\u001b[0m\n\u001b[1;32m   1784\u001b[0m na_values \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mna_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1785\u001b[0m skiprows \u001b[38;5;241m=\u001b[39m options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskiprows\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1787\u001b[0m validate_header_arg(options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_col \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe value of index_col couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:233\u001b[0m, in \u001b[0;36mvalidate_header_arg\u001b[0;34m(header)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a bool to header is invalid. Use header=None for no header or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader=int or list-like of ints to specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe row(s) making up the column names\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# GH 16338\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader must be integer or list of integers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: header must be integer or list of integers"
     ]
    }
   ],
   "source": [
    "df = extract_csv_from_zip(zip_path, extract_filename, delimiter=\";\", header=\"none\", names=DEFAULT_HEADERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e78d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
