{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ffcc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "98c0c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define headers for CSV files without headers\n",
    "DEFAULT_HEADERS = [\n",
    "    \"AÃ‘O\", \"FECHA_DEF\", \"SEXO_NOMBRE\", \"EDAD_TIPO\", \"EDAD_CANT\", \"COD_COMUNA\", \"COMUNA\", \"NOMBRE_REGION\",\n",
    "    \"DIAG1\", \"CAPITULO_DIAG1\", \"GLOSA_CAPITULO_DIAG1\", \"CODIGO_GRUPO_DIAG1\", \"GLOSA_GRUPO_DIAG1\",\n",
    "    \"CODIGO_CATEGORIA_DIAG1\", \"GLOSA_CATEGORIA_DIAG1\", \"CODIGO_SUBCATEGORIA_DIAG1\", \"GLOSA_SUBCATEGORIA_DIAG1\",\n",
    "    \"DIAG2\", \"CAPITULO_DIAG2\", \"GLOSA_CAPITULO_DIAG2\", \"CODIGO_GRUPO_DIAG2\", \"GLOSA_GRUPO_DIAG2\",\n",
    "    \"CODIGO_CATEGORIA_DIAG2\", \"GLOSA_CATEGORIA_DIAG2\", \"CODIGO_SUBCATEGORIA_DIAG2\", \"GLOSA_SUBCATEGORIA_DIAG2\",\n",
    "    \"LUGAR_DEFUNCION\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2f26ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_paths():\n",
    "    # Check if running in the GitHub Actions environment\n",
    "    if 'GITHUB_ACTIONS' in os.environ:\n",
    "        base_path = os.getcwd()\n",
    "    else:\n",
    "        # Assuming your notebook is in the 'scripts' directory\n",
    "        base_path = os.path.abspath(os.path.join(os.getcwd(), '../../../'))\n",
    "\n",
    "    source_path = os.path.join(base_path, \"data/source/minsal/deis\")\n",
    "    processed_path = os.path.join(base_path, \"data/processed/minsal/deis\")\n",
    "    \n",
    "    return source_path, processed_path\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download file from {url}\")\n",
    "\n",
    "def detect_encoding(file_path, num_bytes=1000):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        raw_data = f.read(num_bytes)\n",
    "    result = chardet.detect(raw_data)\n",
    "    return result['encoding']\n",
    "\n",
    "def read_csv_file(file_path, delimiter=';', header='infer', names=None):\n",
    "    encoding = detect_encoding(file_path)\n",
    "    print(f\"Detected encoding for {file_path}: {encoding}\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=delimiter, encoding=encoding, index_col=False, low_memory=False, header=header, names=names)\n",
    "        print(f\"Successfully read file with encoding {encoding}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_csv_from_zip(zip_path, extract_filename, delimiter=';', header='infer', names=None):\n",
    "    with ZipFile(zip_path, 'r') as zip_file:\n",
    "        if extract_filename in zip_file.namelist():\n",
    "            with zip_file.open(extract_filename) as f:\n",
    "                # Save the file temporarily to detect encoding\n",
    "                temp_csv_path = os.path.join(\"/tmp\", extract_filename)\n",
    "                with open(temp_csv_path, 'wb') as temp_f:\n",
    "                    temp_f.write(f.read())\n",
    "                df = read_csv_file(temp_csv_path, delimiter=delimiter, header=header, names=names)\n",
    "                return df\n",
    "        else:\n",
    "            print(f\"{extract_filename} not found in the zip archive.\")\n",
    "            return None\n",
    "\n",
    "def process_zip_file(url, extract_filename, source_dir, processed_dir, delimiter=';', header='infer', names=None):\n",
    "    # Extract filename from URL\n",
    "    zip_filename = url.split('/')[-1]\n",
    "    zip_path = os.path.join(source_dir, zip_filename)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(zip_path):\n",
    "        # Download the zip file if it doesn't exist\n",
    "        download_file(url, zip_path)\n",
    "        print(f\"Downloaded {zip_filename}\")\n",
    "    else:\n",
    "        print(f\"{zip_filename} already exists. Skipping download.\")\n",
    "    \n",
    "    # Extract and process the CSV file\n",
    "    df = extract_csv_from_zip(zip_path, extract_filename, delimiter=delimiter, header=header, names=names)\n",
    "    if df is not None:\n",
    "        parquet_filename = extract_filename.replace('.csv', '.parquet')\n",
    "        parquet_path = os.path.join(processed_dir, parquet_filename)\n",
    "        df.to_parquet(parquet_path)\n",
    "        print(f\"Processed {extract_filename} and saved to {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ebaa142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "source_dir, processed_dir = get_data_paths()\n",
    "os.makedirs(source_dir, exist_ok=True)\n",
    "os.makedirs(processed_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "203fcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file URLs and corresponding CSV filenames to extract\n",
    "file_info = [\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip\", \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv\"),\n",
    "    (\"https://repositoriodeis.minsal.cl/DatosAbiertos/VITALES/DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip\", \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2f481f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.zip\n",
      "Detected encoding for /tmp/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv: ISO-8859-1\n",
      "Successfully read file with encoding ISO-8859-1\n",
      "Processed DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.csv and saved to /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/minsal/deis/DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\n",
      "Downloaded DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.zip\n",
      "Detected encoding for /tmp/DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv: ISO-8859-1\n",
      "Successfully read file with encoding ISO-8859-1\n",
      "Processed DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.csv and saved to /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/minsal/deis/DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.parquet\n"
     ]
    }
   ],
   "source": [
    "# Process each zip file\n",
    "for url, extract_filename in file_info:\n",
    "    header_option = 'infer' if '2022_2024' in extract_filename else None\n",
    "    names_option = None if header_option == 'infer' else DEFAULT_HEADERS\n",
    "    process_zip_file(url, extract_filename, source_dir, processed_dir, header=header_option, names=names_option)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d26db7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame saved to /Users/ernestolaval/Documents/nodeJS/github/datos_abiertos/data/processed/minsal/deis/combined_defunciones.parquet\n"
     ]
    }
   ],
   "source": [
    "# Read and merge both files using the common structure\n",
    "df1 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_2022_2024_25062024.parquet\"))\n",
    "df2 = pd.read_parquet(os.path.join(processed_dir, \"DEFUNCIONES_FUENTE_DEIS_1990_2021_CIFRAS_OFICIALES.parquet\"))\n",
    "\n",
    "# Concatenate dataframes\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save combined dataframe\n",
    "combined_parquet_path = os.path.join(processed_dir, \"combined_defunciones.parquet\")\n",
    "df_combined.to_parquet(combined_parquet_path)\n",
    "print(f\"Combined DataFrame saved to {combined_parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416c586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
